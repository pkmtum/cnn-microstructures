{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Notebook to train several models with different training/data configurations**\n",
        "\n",
        "The notebook is designed to run on colab. It can be easily adapted to run locally, with a few imports."
      ],
      "metadata": {
        "id": "voLj384m8Sq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the path to the helper functions folder\n",
        "# On drive for example: '/content/drive/My Drive/myfolder/helper_functions'\n",
        "modules_path = './helper_functions'\n",
        "models_path = './models' # for the models folder\n",
        "\n",
        "# This is the path to the data folder\n",
        "# On drive for example: '/content/drive/myfolder/data'\n",
        "base_path = './data/'\n",
        "\n",
        "# This is the path to the tensorflow checkpoint folder\n",
        "# On drive for example: '/content/drive/MyDrive/best_models'\n",
        "checkpoint_filepath = './best_models'"
      ],
      "metadata": {
        "id": "_XPz-tTq-pcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMnGpUDUa0CT"
      },
      "outputs": [],
      "source": [
        "# pip calls, COLAB STUFF\n",
        "!pip install tensorflow_addons\n",
        "\n",
        "# all the nice imports <3\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import keras\n",
        "import importlib\n",
        "import pickle\n",
        "\n",
        "# black magic, so that tf.Tensor objects can be used as numpy things\n",
        "# all hail tensorflow 0_0\n",
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "np_config.enable_numpy_behavior()\n",
        "\n",
        "# show important library versions\n",
        "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
        "print(\"TensorFlow Datasets version: \",tfds.__version__)\n",
        "\n",
        "\n",
        "# add custom paths (to import the nice models and helper classes)\n",
        "import sys\n",
        "sys.path.append(models_path)\n",
        "sys.path.append(modules_path)\n",
        "\n",
        "# add custom imports here\n",
        "import processing\n",
        "import metrics\n",
        "import conv_architectures as ca\n",
        "import autoencoder_models as am"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Very Google Colab specific stuff"
      ],
      "metadata": {
        "id": "ddPXggOS9DFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import & mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "  tf.config.experimental_connect_to_cluster(tpu)\n",
        "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "  tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "except ValueError:\n",
        "  print('Warning: TPU device not found')\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print('Warning: GPU device not found')\n",
        "else:  \n",
        "  print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "\n",
        "# Tensorboard stuff (callbacks for logging data)\n",
        "%load_ext tensorboard\n",
        "logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)"
      ],
      "metadata": {
        "id": "U_C4PGzO9B7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up of runs\n",
        "**Walkthorugh**\n",
        "1. Give the problem a name for file naming and create the PROBLEM FOLDER, along with a sub-folder called \"history\" \n",
        "2. The data sets are named based on sample size (size) and data set size (dataset_size). These can be specified below\n",
        "3. All parameters that can be varied (or that there exists data sets for)\n",
        "4. Custom definitions, and defintions of the models (dictionary with: model class, designation, and regularization)"
      ],
      "metadata": {
        "id": "WKCkGzHN9NXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.\n",
        "PROBLEM = 'DS'\n",
        "PROBLEM_FOLDER = 'my_experiments_folder'\n",
        "\n",
        "# 2.\n",
        "dataset_size = '10000'\n",
        "size = '32' # or 32 or 64\n",
        "\n",
        "size_path = size + 'x' + size + '/'\n",
        "model_base_path = './'+ PROBLEM_FOLDER + '/' + size_path\n",
        "\n",
        "\n",
        "# 3.\n",
        "cr_array = np.array(['0.1000', '0.0500', '0.0200', '0.0100', '0.0050', '0.0020']) # contrast_ratios\n",
        "ds_size_array = np.array([1000,2000,5000,10000]) # data set sizes\n",
        "twopoint_array = np.array([False, True]) # using two-point correlations\n",
        "\n",
        "# 4.\n",
        "model_array = np.array([\n",
        "    {'model' : ca.Conv32Model1, 'name' : 'large', 'reg': 0.001},\n",
        "    # {'model' : ca.ConvModel3, 'name' : 'medium', 'reg': 0.005},\n",
        "    {'model' : ca.Conv32Model3, 'name' : 'small', 'reg': 0.01},\n",
        "    ])\n",
        "\n",
        "cr_array = np.array(['0.0100'])\n",
        "ds_size_array = np.array([1000, 2000, 5000])\n",
        "twopoint_array = np.array([False])\n",
        "EPOCHS = 1000\n",
        "REG = 0.001\n",
        "\n"
      ],
      "metadata": {
        "id": "L6FW0eYd92ID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execute Runs"
      ],
      "metadata": {
        "id": "N6_yz3949Q8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_array = []\n",
        "FILENAME = 'all_data_' + PROBLEM\n",
        "TEST_SIZE = 1000\n",
        "SPLITS = np.array([0.9,0,0.1])\n",
        "\n",
        "counter = 0\n",
        "\n",
        "for model_data in model_array:\n",
        "  for cr in cr_array:\n",
        "    for ds in ds_size_array:\n",
        "      for tp in twopoint_array:\n",
        "\n",
        "        counter += 1\n",
        "        print(\"Currently working on model No.\", counter)\n",
        "\n",
        "        data_path = base_path + size_path + 'n=' + size + '_x=' + dataset_size + '_cr=' + cr + '_DS.h5'\n",
        "        tp_data_path = base_path + size_path + 'n=' + size + '_x=' + dataset_size + '_cr=' + cr + '_TP.h5'\n",
        "\n",
        "        f = h5py.File(data_path, 'r')\n",
        "        imgs = np.array(f['images'])\n",
        "        labs = np.array(f['c_vectors'])\n",
        "\n",
        "        print(labs[0])\n",
        "\n",
        "        twopoint_f = h5py.File(tp_data_path, 'r')\n",
        "        tp_imgs = np.array(twopoint_f['twopoint'])\n",
        "\n",
        "        f.close()\n",
        "        twopoint_f.close()\n",
        "\n",
        "        if tp:\n",
        "          ims = tp_imgs\n",
        "\n",
        "\n",
        "        dataproc = processing.Processing(\n",
        "          imgs[:ds], \n",
        "          labs[:ds], \n",
        "          SPLITS, \n",
        "          custom_valid = (imgs[-1000:-500], labs[-1000:-500]),\n",
        "          custom_test = (imgs[-500:], labs[-500:]),\n",
        "          batch_size=256, \n",
        "          conv_behavior=True, \n",
        "          scale_labels=False, \n",
        "          symmetric=True, \n",
        "          twopoint=False,\n",
        "          shuffle=False,\n",
        "          pca=False,\n",
        "          # pca_variance=0.7\n",
        "          )\n",
        "        \n",
        "        model = model_data['model'](dataproc, regularization=model_data['reg'])\n",
        "        print(model.summary())\n",
        "\n",
        "        \n",
        "        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "            filepath=checkpoint_filepath,\n",
        "            save_weights_only=True,\n",
        "            monitor='val_r_square',\n",
        "            mode='max',\n",
        "            save_best_only=True)\n",
        "\n",
        "\n",
        "        params = {\n",
        "            'x' : dataproc.ds_train,\n",
        "            'batch_size' : dataproc.batch_size, \n",
        "            'epochs' : EPOCHS, \n",
        "            'validation_data' : dataproc.ds_val, \n",
        "            'verbose' : 1, \n",
        "            'callbacks' : [model_checkpoint_callback]\n",
        "            # 'callbacks' : [tensorboard_callback, model_checkpoint_callback]\n",
        "        }\n",
        "\n",
        "        model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[tfa.metrics.r_square.RSquare()])\n",
        "        history = model.fit(**params)\n",
        "\n",
        "        model.load_weights(checkpoint_filepath)\n",
        "\n",
        "        test_metrics = model.evaluate(dataproc.ds_test)\n",
        "\n",
        "        custom_path = PROBLEM + '_model' + model_data['name'] + '_ds' + str(dataproc.splits[0]) + '_tp' + str(tp) + '_epochs' + str(params['epochs']) + '_cr' + cr\n",
        "        total_path = model_base_path + custom_path\n",
        "        model.save(total_path)\n",
        "\n",
        "        results_array.append(\n",
        "            {\n",
        "                'run_name' : custom_path,\n",
        "                'model_name' : model_data['name'],\n",
        "                'regularization' : model_data['reg'],\n",
        "                'ds_size' : dataproc.splits[0],\n",
        "                'twopoint' : tp,\n",
        "                'contrast_ratio' : cr,\n",
        "                'epochs' : params['epochs'],\n",
        "                'history' : history.history,\n",
        "                'test_metrics' : test_metrics,\n",
        "                'model_function' : model_data['model'],\n",
        "                'path_to_weights' : total_path,\n",
        "            }\n",
        "        )\n",
        "\n",
        "\n",
        "with open(model_base_path + 'history/' + FILENAME, 'wb') as file_pi:\n",
        "  pickle.dump(results_array, file_pi)\n"
      ],
      "metadata": {
        "id": "qYasOUyMcJQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs/scalars"
      ],
      "metadata": {
        "id": "untuOdvWCwsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Examples and use cases"
      ],
      "metadata": {
        "id": "kW1uWHr39sr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EXAMPLE: Reopen files\n",
        "\n",
        "mymod = ca.ConvModel2(dataproc)\n",
        "print(mymod.summary())\n",
        "\n",
        "mymod.load_weights(model_base_path + 'modelmedium_ds4250_tpTrue_epochs5')\n",
        "\n",
        "mymod.compile(optimizer=\"Adam\", loss=\"mse\", metrics='accuracy')\n",
        "mymod.evaluate(dataproc.ds_test)"
      ],
      "metadata": {
        "id": "Z436lkYUp25K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EXAMPLE: Plotting data\n",
        "\n",
        "with open(PROBLEM_FOLDER + 'history/all_data_cr', 'rb') as f:\n",
        "    results = pickle.load(f)\n",
        "\n",
        "length = len(results[0]['history']['val_r_square'])\n",
        "epochs = np.linspace(1,length, num=length)\n",
        "\n",
        "for res in results:\n",
        "  # print(res['history'])\n",
        "  mn = res['model_name'].upper()[0] + str(res['twopoint']) + \"_cr\" + res['contrast_ratio'] + '_ds' + str(res['ds_size'])\n",
        "  y = res['history']['val_r_square']\n",
        "  if res['model_name'] == 'small' or True:\n",
        "    print(mn, \"Average value\", np.mean(y), \"max val\", np.max(y))\n",
        "    plt.plot(epochs,y, label=mn)\n",
        "\n",
        "plt.legend(loc='lower right')\n",
        "ax = plt.gca()\n",
        "ax.set_xlim([0, length])\n",
        "ax.set_ylim([0.3, 1])\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "icXoo68HHnKl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}