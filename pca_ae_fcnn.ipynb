{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Notebook made to compare Autoencoders with PCA using FCNN**\n",
        "\n",
        "The notebook is designed to run on colab. It can be easily adapted to run locally, with a few imports."
      ],
      "metadata": {
        "id": "2XaSoKV04gcx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up"
      ],
      "metadata": {
        "id": "81kLEimSTc9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the path to the helper functions folder\n",
        "# On drive for example: '/content/drive/My Drive/myfolder/helper_functions'\n",
        "modules_path = './helper_functions'\n",
        "models_path = './models' # for the models folder\n",
        "\n",
        "# This is the path to the data folder\n",
        "# On drive for example: '/content/drive/myfolder/data'\n",
        "base_path = './data/'\n",
        "\n",
        "# This is the path to the tensorflow checkpoint folder\n",
        "# On drive for example: '/content/drive/MyDrive/best_models'\n",
        "checkpoint_filepath = './best_models'"
      ],
      "metadata": {
        "id": "NA0P30-iSsQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxk8bLnG4dfx"
      },
      "outputs": [],
      "source": [
        "# pip calls\n",
        "!pip install tensorflow_addons\n",
        "\n",
        "# all the nice imports <3\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import keras\n",
        "import importlib\n",
        "import pickle\n",
        "\n",
        "# black magic, so that tf.Tensor objects can be used as numpy things\n",
        "# all hail tensorflow 0_0\n",
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "np_config.enable_numpy_behavior()\n",
        "\n",
        "# show important library versions\n",
        "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
        "print(\"TensorFlow Datasets version: \",tfds.__version__)\n",
        "\n",
        "\n",
        "checkpoint_dir = os.path.dirname(checkpoint_filepath)\n",
        "\n",
        "# add custom paths (to import the nice models and helper classes)\n",
        "import sys\n",
        "sys.path.append(models_path)\n",
        "sys.path.append(modules_path)\n",
        "\n",
        "# add custom imports here\n",
        "import processing\n",
        "import metrics\n",
        "import first_models as fm\n",
        "import autoencoder_models as am\n",
        "import conv_architectures as ca"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Very specific Colab things"
      ],
      "metadata": {
        "id": "NBlFwVBvTevj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import & mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# hardware speed-up magic (TPU OR GPU?)\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print('Warning: GPU device not found')\n",
        "else:  \n",
        "  print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "# Tensorboard stuff (callbacks for logging data)\n",
        "%load_ext tensorboard\n",
        "logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)"
      ],
      "metadata": {
        "id": "ItxiVT5vTboO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up of runs\n",
        "**Walkthorugh**\n",
        "1. Give the problem a name for file naming and create the PROBLEM FOLDER, along with a sub-folder called \"history\" \n",
        "2. The data sets are named based on sample size (size) and data set size (dataset_size). These can be specified below\n",
        "3. All parameters that can be varied (or that there exists data sets for)\n",
        "4. Custom definitions, and defintions of the models (dictionary with: model class, designation, and regularization)"
      ],
      "metadata": {
        "id": "zUWLm0oW847I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.\n",
        "PROBLEM_FOLDER = 'pca_ae_comparison'\n",
        "PROBLEM = 'HAM'\n",
        "\n",
        "# 2. \n",
        "dataset_size = '1000'\n",
        "size = '32' # or 32 or 64\n",
        "\n",
        "size_path = size + 'x' + size + '/'\n",
        "model_base_path = './'+ PROBLEM_FOLDER + '/' + size_path\n",
        "\n",
        "# 3.\n",
        "# LOOP EVERYTHING (Models are missing)\n",
        "cr_array = np.array(['0.1000', '0.0500', '0.0200', '0.0100', '0.0050', '0.0020'])\n",
        "ds_size_array = np.array([1000,2000,5000,10000])\n",
        "twopoint_array = np.array([False, True])\n",
        "\n",
        "# CUSTOM LOOPS\n",
        "# CAN VARY: contrast ratio, ds size, 2pc, models\n",
        "\n",
        "# choose models, for example\n",
        "PATH_TWOPOINT = './autoencoders/32x32/AUTOENCODER_ds18000_tpTrue_epochs1000'\n",
        "PATH_BINARY = './autoencoders/32x32/AUTOENCODER_ds18000_tpFalse_epochs1000'\n",
        "\n",
        "type_array = np.array(['pretrained', 'untrained'])\n",
        "# type_array = np.array(['freeE', 'frozenE'])\n",
        "cr_array = np.array(['0.0100'])\n",
        "ds_size_array = np.array([1000])\n",
        "twopoint_array = np.array([False])\n",
        "EPOCHS = 1000\n",
        "REG = 0.001\n"
      ],
      "metadata": {
        "id": "AFWxXvXE9Jl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function to define the FCNN**"
      ],
      "metadata": {
        "id": "tkh1cKeGUaMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model2(model_type, encoder_path=None, regularization=0.05):\n",
        "    reg2 = tf.keras.regularizers.L2\n",
        "    REG = regularization\n",
        "    encoder = am.create_enc_spec(inputshape=(32,32,1))\n",
        "    if model_type == 'pretrained':\n",
        "      print(\"Creating Pretrained Model\")\n",
        "      encoder.load_weights(encoder_path)\n",
        "      # TOGGLE TRAINABILITY OF AUTOENCODER\n",
        "      # encoder.trainable = False\n",
        "    elif model_type == 'untrained':\n",
        "      print(\"Creating Untrained Model\")\n",
        "    else:\n",
        "      print(\"Unknown model type\")\n",
        "      return 42\n",
        "    \n",
        "    # DEFINE FCNN HERE\n",
        "    # change input shape in (x, x, 1)\n",
        "    inputs = tf.keras.Input(shape=(32,32,1))\n",
        "    conv_out = encoder(inputs)\n",
        "    y1 = tf.keras.layers.Flatten()(conv_out)\n",
        "    y4 = tf.keras.layers.Dense(6, activation='relu', kernel_regularizer=reg2(REG))(y1)\n",
        "\n",
        "    return tf.keras.Model(inputs=[inputs], outputs=[y4])"
      ],
      "metadata": {
        "id": "Qrli1wCP1gQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execute Runs"
      ],
      "metadata": {
        "id": "iU0mZQ-gVa2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 1000\n",
        "results_array = []\n",
        "FILENAME = 'all_data_' + PROBLEM\n",
        "TEST_SIZE = 1000\n",
        "SPLITS = np.array([0.8, 0.1, 0.1])\n",
        "\n",
        "counter = 0\n",
        "\n",
        "for mt in type_array:\n",
        "  for cr in cr_array:\n",
        "    for ds in ds_size_array:\n",
        "      for tp in twopoint_array:\n",
        "\n",
        "        counter += 1\n",
        "        print(\"Currently working on model No.\", counter)\n",
        "\n",
        "        data_path = base_path + size_path + 'n=' + size + '_x=' + dataset_size + '_cr=' + cr + '_DS.h5'\n",
        "        tp_data_path = base_path + size_path + 'n=' + size + '_x=' + dataset_size + '_cr=' + cr + '_TP.h5'\n",
        "\n",
        "        f = h5py.File(data_path, 'r')\n",
        "        imgs = np.array(f['images'])\n",
        "        labs = np.array(f['c_vectors'])\n",
        "\n",
        "        print(labs[0])\n",
        "\n",
        "        twopoint_f = h5py.File(tp_data_path, 'r')\n",
        "        tp_imgs = np.array(twopoint_f['twopoint'])\n",
        "\n",
        "        f.close()\n",
        "        twopoint_f.close()\n",
        "\n",
        "        if tp:\n",
        "          ims = tp_imgs\n",
        "\n",
        "        if mt == 'PCA':\n",
        "          pca_ind = True\n",
        "        else:\n",
        "          pca_ind = False\n",
        "\n",
        "\n",
        "        dataproc = processing.Processing(\n",
        "          imgs[:ds], \n",
        "          labs[:ds], \n",
        "          SPLITS, \n",
        "          custom_valid = (imgs[-200:-100], labs[-200:-100]),\n",
        "          custom_test = (imgs[-100:], labs[-100:]),\n",
        "          batch_size=256, \n",
        "          conv_behavior=not pca_ind, \n",
        "          scale_labels=False, \n",
        "          symmetric=True, \n",
        "          twopoint=False,\n",
        "          shuffle=False,\n",
        "          pca=pca_ind,\n",
        "          pca_dims=128\n",
        "          )\n",
        "\n",
        "        if tp:\n",
        "          xpath = PATH_TWOPOINT\n",
        "        else:\n",
        "          xpath = PATH_BINARY\n",
        "\n",
        "\n",
        "        model = get_model2(mt, xpath, regularization=0.02)\n",
        "        print(model.summary())\n",
        "\n",
        "        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "            filepath=checkpoint_filepath,\n",
        "            save_weights_only=True,\n",
        "            monitor='val_r_square',\n",
        "            mode='max',\n",
        "            save_best_only=True)\n",
        "\n",
        "\n",
        "        params = {\n",
        "            'x' : dataproc.ds_train,\n",
        "            'batch_size' : dataproc.batch_size, \n",
        "            'epochs' : EPOCHS, \n",
        "            'validation_data' : dataproc.ds_val, \n",
        "            'verbose' : 1, \n",
        "            'callbacks' : [model_checkpoint_callback]\n",
        "            # 'callbacks' : [tensorboard_callback, model_checkpoint_callback]\n",
        "        }\n",
        "\n",
        "        model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[tfa.metrics.r_square.RSquare()])\n",
        "        history = model.fit(**params)\n",
        "\n",
        "        model.load_weights(checkpoint_filepath)\n",
        "\n",
        "        test_metrics = model.evaluate(dataproc.ds_test)\n",
        "\n",
        "        custom_path = PROBLEM + '_model' + mt + '_ds' + str(dataproc.splits[0]) + '_tp' + str(tp) + '_epochs' + str(params['epochs']) + '_cr' + cr\n",
        "        total_path = model_base_path + custom_path\n",
        "        model.save(total_path)\n",
        "\n",
        "        results_array.append(\n",
        "            {\n",
        "                'run_name' : custom_path,\n",
        "                'model_name' : mt,\n",
        "                'regularization' : 0,\n",
        "                'ds_size' : dataproc.splits[0],\n",
        "                'twopoint' : tp,\n",
        "                'contrast_ratio' : cr,\n",
        "                'epochs' : params['epochs'],\n",
        "                'history' : history.history,\n",
        "                'test_metrics' : test_metrics,\n",
        "                'model_function' : model, # the actual model here\n",
        "                'path_to_weights' : total_path,\n",
        "            }\n",
        "        )\n",
        "\n",
        "\n",
        "with open(model_base_path + 'history/' + FILENAME, 'wb') as file_pi:\n",
        "  pickle.dump(results_array, file_pi)"
      ],
      "metadata": {
        "id": "cxqpB-iY84jP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}